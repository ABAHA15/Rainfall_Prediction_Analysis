# -*- coding: utf-8 -*-
"""Rainfall_Prediction_using_Machine_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-YHnQOo7jQHE5j_i1PrXZYO7joCFpE5

**Importing the dependencies**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import SimpleImputer
import pickle

"""**Data Collection and Processing**"""

# laod the dataset to a pandas dataframe
data = pd.read_csv("/content/Rainfall.csv")

print(type(data))

data.shape

data.head()

data.tail()

data["day"].unique()

print("Data Info:")
data.info()

data.columns

# remove extra  spaces in all columns
data.columns = data.columns.str.strip()

data.columns

print("Data Info:")
data.info()

data = data.drop(columns=["day"])

data.head()

# checking the number of missing values
print(data.isnull().sum())

data["winddirection"].unique()

# handle missing values
data["winddirection"] = data["winddirection"].fillna(data["winddirection"].mode()[0])
data["windspeed"] = data["windspeed"].fillna(data["windspeed"].median())

# checking the number of missing values
print(data.isnull().sum())

data["rainfall"].unique()

# converting the yes & no to 1 and 0 respectively
data["rainfall"] = data["rainfall"].map({"yes": 1, "no": 0})

data.head()

"""**Exploratory Data Analysis (EDA)**"""

data.shape

# setting plot style for all the plots
sns.set(style="whitegrid")

data.describe()

data.columns

plt.figure(figsize=(15, 10))

for i, column in enumerate(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity','cloud', 'sunshine', 'windspeed'], 1):
  plt.subplot(3, 3, i)
  sns.histplot(data[column], kde=True)
  plt.title(f"Distribution of {column}")

plt.tight_layout()
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x="rainfall", data=data)
plt.title("Distribution of Rainfall")
plt.show()

# correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation heatmap")
plt.show()

plt.figure(figsize=(15, 10))

for i, column in enumerate(['pressure', 'maxtemp', 'temparature', 'mintemp', 'dewpoint', 'humidity','cloud', 'sunshine', 'windspeed'], 1):
  plt.subplot(3, 3, i)
  sns.boxplot(data[column])
  plt.title(f"Boxplot of {column}")

plt.tight_layout()
plt.show()

"""**Data Preprocessing**"""

# drop highly correlated column
data = data.drop(columns=['maxtemp', 'temparature', 'mintemp'])

data.head()

print(data["rainfall"].value_counts())

# separate majority and minority class
df_majority = data[data["rainfall"] == 1]
df_minority = data[data["rainfall"] == 0]

print(df_majority.shape)
print(df_minority.shape)

# downsample majority class to match minority count
df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)

df_majority_downsampled.shape

df_downsampled = pd.concat([df_majority_downsampled, df_minority])

df_downsampled.shape

df_downsampled.head()

# shuffle the final dataframe
df_downsampled = df_downsampled.sample(frac=1, random_state=42).reset_index(drop=True)

df_downsampled.head()

df_downsampled["rainfall"].value_counts()

# split features and target as X and y
X = df_downsampled.drop(columns=["rainfall"])
y = df_downsampled["rainfall"]

print(X)

print(y)

# splitting the data into training data and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""**Model Training**"""

# Logistic regression
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)
lr_preds = lr_model.predict(X_test)
print("Logistic Regression Results:")
print(classification_report(y_test, lr_preds))

# SVM
svm_model = SVC(probability=True)
svm_model.fit(X_train, y_train)
svm_preds = svm_model.predict(X_test)
print("SVM Results:")
print(classification_report(y_test, svm_preds))

# kNN
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
knn_preds = knn_model.predict(X_test)
print("KNN Results:")
print(classification_report(y_test, knn_preds))

# Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
print("Random Forest Results:")
print(classification_report(y_test, rf_preds))

# XGB classifier
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)
print("XGBoost Results:")
print(classification_report(y_test, xgb_preds))

# Train a Decision Tree model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_preds = dt_model.predict(X_test)
print("Decision Tree Results:")
print(classification_report(y_test, dt_preds))

"""**Model Evaluation**"""

# Evaluate all models on test data
# Compare metrics
models = {
    "Random Forest": rf_model,
    "Logistic Regression": lr_model,
    "SVM": svm_model,
    "KNN": knn_model,
    "XGBoost": xgb_model,
    "Decision Tree": dt_model
}

results = {}
for name, model in models.items():
    preds = model.predict(X_test)
    roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) if hasattr(model, "predict_proba") else None
    results[name] = {
        "Accuracy": accuracy_score(y_test, preds),
        "F1-Score": classification_report(y_test, preds, output_dict=True)["weighted avg"]["f1-score"],
        "ROC-AUC": roc_auc
    }

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

"""**Refining** **the** **best** **model**"""

# Hyperparameter tuning using grid search
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, scoring='f1', cv=5)
grid_search.fit(X_train, y_train)
print("Best Parameters:", grid_search.best_params_)

"""**Prediction on unknown data**"""

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)

input_df = pd.DataFrame([input_data], columns=['pressure', 'dewpoint', 'humidity', 'cloud', 'sunshine','winddirection', 'windspeed'])

input_df

prediction = lr_model.predict(input_df)
print("Prediction result:", "Rainfall" if prediction[0] == 1 else "No Rainfall")

print(prediction)

prediction[0]

# save model and feature names to a pickle file
model_data = {"model": model, "feature_names": X.columns.tolist()}

with open("rainfall_prediction_model.pkl", "wb") as file:
  pickle.dump(model_data, file)

"""**Load the saved model and file and use it for prediction**"""

import pickle
import pandas as pd

# load the trained model and feature names from the pickle file
with open("rainfall_prediction_model.pkl", "rb") as file:
  model_data = pickle.load(file)

model = model_data["model"]
feature_names = model_data["feature_names"]

input_data = (1015.9, 19.9, 95, 81, 0.0, 40.0, 13.7)

input_df = pd.DataFrame([input_data], columns=feature_names)

prediction = model.predict(input_df)
print("Prediction result:", "Rainfall" if prediction[0] == 1 else "No Rainfall")